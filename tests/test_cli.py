"""tests for the cli interface to studies"""

import builtins
import filecmp
import glob
import io
import os
import pathlib
import shutil
import sys
import tomllib
import zipfile
from contextlib import contextmanager
from contextlib import nullcontext as does_not_raise
from pathlib import Path
from unittest import mock

import duckdb
import pandas
import pytest
import requests
import responses
import time_machine

from cumulus_library import __version__, cli, databases, errors
from tests.conftest import duckdb_args

FHIR_RESOURCE_TABLE_COUNT = 19


@contextmanager
def mock_stdin(value: str):
    stdin = sys.stdin
    sys.stdin = value
    yield
    sys.stdin = stdin


@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_cli_invalid_study(tmp_path):
    with pytest.raises(SystemExit):
        args = duckdb_args(["build", "-t", "foo"], tmp_path)
        cli.main(cli_args=args)


@mock.patch.dict(
    os.environ,
    clear=True,
)
@pytest.mark.parametrize(
    "args",
    [
        ([]),
        (["-t", "all"]),
    ],
)
def test_cli_early_exit(args):
    with pytest.raises(SystemExit):
        cli.main(cli_args=args)


@mock.patch.dict(
    os.environ,
    clear=True,
)
@mock.patch("json.load")
@pytest.mark.parametrize(
    "args,raises,expected",
    [
        (
            [
                "build",
                "-t",
                "core",
            ],
            does_not_raise(),
            "core__condition",
        ),
        (
            ["build", "-t", "study_python_valid"],
            does_not_raise(),
            "study_python_valid__table",
        ),
        (
            ["build", "-t", "study_python_valid", "--builder", "module2"],
            does_not_raise(),
            "study_python_valid__count_table",
        ),
        (
            ["build", "-t", "study_python_valid", "--builder", "not_a_module"],
            pytest.raises(SystemExit),
            "study_python_valid__count_table",
        ),
        (
            ["build", "-t", "study_bad_manifest"],
            pytest.raises(SystemExit),
            "study_python_valid__count_table",
        ),
        (["build", "-t", "wrong"], pytest.raises(SystemExit), None),
        (["build"], pytest.raises(SystemExit), None),
        (
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                f"{Path(__file__).resolve().parents[0]}/test_data/study_valid",
                "--database",
                "test",
            ],
            does_not_raise(),
            "study_valid__table",
        ),
    ],
)
def test_cli_path_mapping(mock_load_json, monkeypatch, tmp_path, args, raises, expected):
    with raises:
        monkeypatch.syspath_prepend(f"{Path(__file__).resolve().parents[0]}/test_data/")
        mock_load_json.return_value = {
            "__desc__": "",
            "allowlist": [
                "study_python_valid",
                "study_bad_manifest",
            ],
        }
        args = duckdb_args(args, tmp_path)
        cli.main(cli_args=args)
        db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
        db.connect()
        assert (expected,) in db.cursor().execute("show tables").fetchall()


@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_count_builder_mapping(tmp_path):
    with does_not_raise():
        args = duckdb_args(
            [
                "build",
                "-t",
                "study_python_counts_valid",
                "-s",
                f"{Path(__file__).resolve().parents[0]}/test_data/",
            ],
            tmp_path,
        )
        cli.main(cli_args=args)
        db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
        db.connect()
        tables = db.cursor().execute("show tables").fetchall()
        for table in [
            ("study_python_counts_valid__lib_transactions",),
            ("study_python_counts_valid__table1",),
            ("study_python_counts_valid__table2",),
        ]:
            assert table in tables


@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_generate_sql(tmp_path):
    with does_not_raise():
        shutil.copytree(
            f"{Path(__file__).resolve().parents[0]}/test_data/study_python_valid",
            f"{tmp_path}/study_python_valid/",
        )
        args = duckdb_args(
            [
                "generate-sql",
                "-t",
                "study_python_valid",
                "-s",
                f"{tmp_path}",
            ],
            tmp_path,
        )
        cli.main(cli_args=args)
        files = glob.glob(f"{tmp_path}/study_python_valid/reference_sql/**")
        assert len(files) == 2
        assert "module1.sql" in ",".join(files)
        for file in files:
            if file.endswith("module1.sql"):
                with open(file) as f:
                    query = "\n".join(line.rstrip() for line in f)
        assert "This sql was autogenerated" in query
        assert "CREATE TABLE IF NOT EXISTS study_python_valid__table" in query


@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_generate_md(tmp_path):
    with does_not_raise():
        shutil.copytree(
            f"{Path(__file__).resolve().parents[0]}/test_data/study_python_valid",
            f"{tmp_path}/study_python_valid/",
        )
        args = duckdb_args(
            [
                "build",
                "-t",
                "study_python_valid",
                "-s",
                f"{tmp_path}",
            ],
            tmp_path,
        )

        cli.main(cli_args=args)
        args = duckdb_args(
            [
                "generate-md",
                "-t",
                "study_python_valid",
                "-s",
                f"{tmp_path}",
            ],
            tmp_path,
        )
        cli.main(cli_args=args)
        test_file = f"{tmp_path}/study_python_valid/study_python_valid_generated.md"
        ref_file = (
            pathlib.Path(__file__).resolve().parent / "test_data/study_python_valid_generated.md"
        )
        assert filecmp.cmp(test_file, ref_file, shallow=True)


@mock.patch.dict(
    os.environ,
    clear=True,
)
@pytest.mark.parametrize(
    "args,expected,raises",
    [
        (
            [
                "clean",
                "-t",
                "core",
            ],
            "core__",
            does_not_raise(),
        ),
        (
            [
                "clean",
                "--prefix",
                "-t",
                "foo",
            ],
            "foo",
            does_not_raise(),
        ),
        (
            [
                "clean",
                "-t",
                "core",
                "--statistics",
            ],
            "core__",
            does_not_raise(),
        ),
        (
            [
                "clean",
            ],
            "core__",
            pytest.raises(SystemExit),
        ),
    ],
)
def test_clean(tmp_path, args, expected, raises):
    with raises:
        cli.main(cli_args=duckdb_args(["build", "-t", "core"], tmp_path))
        with does_not_raise():
            with mock.patch.object(builtins, "input", lambda _: "y"):
                cli.main(cli_args=duckdb_args(args, tmp_path))
                db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
                db.connect()
                for table in db.cursor().execute("show tables").fetchall():
                    assert expected not in table


@mock.patch.dict(
    os.environ,
    clear=True,
)
@pytest.mark.parametrize(
    "build_args,export_args,expected_tables,raises,expected_missing",
    [
        (
            ["build", "-t", "core"],
            ["export", "-t", "core"],
            82,
            does_not_raise(),
            [],
        ),
        (
            # checking that a study is loaded from a child directory
            # of a user-defined path
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                "tests/test_data/",
            ],
            ["export", "-t", "study_valid", "-s", "tests/test_data/"],
            4,
            does_not_raise(),
            [
                "study_valid__table",
                "study_valid__table2",
            ],
        ),
        (
            # checking that a study is loaded from a child directory
            # of a user-defined path
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                "tests/test_data/",
            ],
            ["export", "-t", "study_valid", "-s", "tests/test_data/"],
            4,
            does_not_raise(),
            [
                "study_valid__table",
                "study_valid__table2",
            ],
        ),
        (
            # checking that a study is loaded from the directory of a user-defined
            # path. we're also validating that the CLI accepts the statistics keyword
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                "tests/test_data/study_valid/",
                "--statistics",
            ],
            ["export", "-t", "study_valid", "-s", "tests/test_data/study_valid/"],
            4,
            does_not_raise(),
            [
                "study_valid__table",
                "study_valid__table2",
            ],
        ),
        (
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                "tests/test_data/study_valid/",
                "--statistics",
            ],
            ["export", "-t", "study_valid", "-s", "tests/test_data/study_valid/"],
            4,
            does_not_raise(),
            [
                "study_valid__table",
                "study_valid__table2",
                "study_valid__table",
            ],
        ),
        (
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                "tests/test_data/study_valid/",
                "--continue",
                "test2",
            ],
            ["export", "-t", "study_valid", "-s", "tests/test_data/study_valid/"],
            3,
            pytest.raises(duckdb.CatalogException),
            [],
        ),
        (
            [
                "build",
                "-t",
                "study_valid_parallel",
                "-s",
                "tests/test_data/study_valid_parallel/",
                "--continue",
                "test2",
            ],
            ["export", "-t", "study_valid_parallel", "-s", "tests/test_data/study_valid_parallel/"],
            5,
            does_not_raise(),
            [
                "study_valid_parallel__table",
                "study_valid_parallel__table2",
                "study_valid_parallel__table3",
            ],
        ),
        (
            [
                "build",
                "-t",
                "study_valid_parallel",
                "-s",
                "tests/test_data/study_valid_parallel/",
                "--continue",
                "test3",
            ],
            ["export", "-t", "study_valid_parallel", "-s", "tests/test_data/study_valid_parallel/"],
            5,
            pytest.raises(duckdb.CatalogException),
            [],
        ),
        (
            [
                "build",
                "-t",
                "study_valid_parallel",
                "-s",
                "tests/test_data/study_valid_parallel/",
                "--continue",
                "test4",
            ],
            ["export", "-t", "study_valid_parallel", "-s", "tests/test_data/study_valid_parallel/"],
            5,
            pytest.raises(errors.StudyManifestParsingError),
            [],
        ),
        (
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                "tests/test_data/study_valid/",
                "--continue",
                "test3",
            ],
            ["export", "-t", "study_valid", "-s", "tests/test_data/study_valid/"],
            3,
            pytest.raises(errors.StudyManifestParsingError),
            [],
        ),
        (
            [
                "build",
                "-t",
                "study_dedicated_schema",
                "-s",
                "tests/test_data/study_dedicated_schema/",
            ],
            [
                "export",
                "-t",
                "study_dedicated_schema",
                "-s",
                "tests/test_data/study_dedicated_schema/",
            ],
            5,
            does_not_raise(),
            ["study_dedicated_schema__table_raw_sql"],
        ),
        (
            [
                "build",
                "-t",
                "study_valid_all_exports",
                "-s",
                "tests/test_data/study_valid_all_exports/",
            ],
            [
                "export",
                "-t",
                "study_valid_all_exports",
                "-s",
                "tests/test_data/study_valid_all_exports/",
            ],
            5,
            does_not_raise(),
            [
                "study_valid_all_exports__tablecount",
                "study_valid_all_exports__tableflat",
                "study_valid_all_exports__tablemeta",
            ],
        ),
        (
            [
                "build",
                "-t",
                "study_invalid_duplicate_exports",
                "-s",
                "tests/test_data/study_invalid_duplicate_exports/",
            ],
            [
                "export",
                "-t",
                "study_invalid_duplicate_exports",
                "-s",
                "tests/test_data/study_invalid_duplicate_exports/",
            ],
            3,
            pytest.raises(errors.StudyManifestParsingError),
            [],
        ),
        (
            [
                "build",
                "-t",
                "study_invalid_unsupported_file",
                "-s",
                "tests/test_data/study_invalid_unsupported_file/",
            ],
            [
                "export",
                "-t",
                "study_invalid_unsupported_file",
                "-s",
                "tests/test_data/study_invalid_unsupported_file/",
            ],
            3,
            pytest.raises(SystemExit),
            [],
        ),
    ],
)
def test_cli_executes_queries(
    tmp_path, build_args, export_args, expected_tables, raises, expected_missing
):
    with raises:
        build_args = duckdb_args(build_args, tmp_path)
        cli.main(cli_args=build_args)
        if export_args is not None:
            export_args = duckdb_args(export_args, tmp_path)
            cli.main(cli_args=export_args)

        db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
        db.connect()
        for name, dataset in db.get_cached_datasets().items():
            db.connection.register(f"{name}", dataset)
        found_tables = db.connection.execute(
            "SELECT table_schema,table_name FROM information_schema.tables"
        ).fetchall()

        assert len(found_tables) == expected_tables + FHIR_RESOURCE_TABLE_COUNT
        for table in found_tables:
            # If a table was created by this run, check it has the study prefix
            if "__" in table[0]:
                assert build_args[2] in table[0]

        if export_args is not None:
            # Expected length if not specifying a study dir
            if len(build_args) == 9:
                manifest_dir = cli.get_study_dict([])[build_args[2]]
            else:
                manifest_dir = cli.get_study_dict([cli.get_abs_path(build_args[4])])[build_args[2]]

            with open(f"{manifest_dir}/manifest.toml", "rb") as file:
                config = tomllib.load(file)
            csv_files = glob.glob(f"{tmp_path}/export/{build_args[2]}/*.csv")
            export_config = config["export_config"]
            for export_list in export_config.values():
                for export_table in export_list:
                    if export_table not in expected_missing:
                        assert any(export_table in x for x in csv_files)


@mock.patch.dict(
    os.environ,
    clear=True,
)
@time_machine.travel("2024-01-01T00:00:00Z", tick=False)
@pytest.mark.parametrize(
    "args,input_txt, raises",
    [
        (["export", "-t", "core", "--archive"], "Y", does_not_raise()),
        (["export", "-t", "core", "--archive"], "N", pytest.raises(SystemExit)),
    ],
)
def test_cli_export_archive(tmp_path, args, input_txt, raises):
    with raises:
        with mock_stdin(io.StringIO(input_txt)):
            build_args = duckdb_args(
                [
                    "build",
                    "-t",
                    "core",
                ],
                tmp_path,
            )
            stats_mock = pathlib.Path(f"{tmp_path}/export/core/stats")
            stats_mock.mkdir(parents=True, exist_ok=True)
            with open(stats_mock / "test.txt", "w") as f:
                f.write("test")
            export_args = duckdb_args(args, tmp_path)
            cli.main(cli_args=build_args)
            cli.main(cli_args=export_args)
            if input_txt == "Y":
                archive = zipfile.ZipFile(tmp_path / "export/core__2024-01-01T00:00:00Z.zip")
                for file in [
                    "core__encounter.archive.parquet",
                    "core__count_encounter_type_month.archive.parquet",
                    "stats/test.txt",
                ]:
                    assert file in archive.namelist()
            else:
                archive = zipfile.ZipFile(tmp_path / "export/core/core.zip")
                for file in [
                    "core__encounter.archive.parquet",
                    "core__count_encounter_type_month.archive.parquet",
                ]:
                    assert file in archive.namelist()
                assert "stats/test.txt" not in archive.namelist()


@mock.patch.dict(
    os.environ,
    clear=True,
)
@pytest.mark.parametrize(
    "study,finishes,raises",
    [
        ("study_valid", True, does_not_raise()),
        (
            "study_invalid_bad_query",
            False,
            pytest.raises(SystemExit),
        ),
    ],
)
def test_cli_transactions(tmp_path, study, finishes, raises):
    with raises:
        args = duckdb_args(
            ["build", "-t", study, "-s", "tests/test_data/"],
            f"{tmp_path}",
        )
        args = [*args[:-1], f"{tmp_path}/{study}_duck.db"]
        cli.main(cli_args=args)
    db = databases.DuckDatabaseBackend(f"{tmp_path}/{study}_duck.db")
    db.connect()
    query = db.cursor().execute(f"SELECT * from {study}__lib_transactions").fetchall()
    assert query[0][2] == "started"
    if finishes:
        assert query[1][2] == "finished"
    else:
        assert query[1][2] == "error"


@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_cli_stats_rebuild(tmp_path):
    """Validates statistics build behavior

    Since this is a little obtuse - we are checking:
    - that stats builds run at all
    - that a results table is created on the first run
    - that a results table is :not: created on the second run
    - that a results table is created when we explicitly ask for one with a CLI flag
    """

    cli.main(cli_args=duckdb_args(["build", "-t", "core"], tmp_path, stats=True))
    arg_list = [
        "build",
        "-s",
        "./tests/test_data",
        "-t",
        "psm_test",
        "--db-type",
        "duckdb",
        "--database",
        f"{tmp_path}/duck.db",
    ]
    cli.main(cli_args=[*arg_list, f"{tmp_path}/export"])
    cli.main(cli_args=[*arg_list, f"{tmp_path}/export"])
    cli.main(cli_args=[*arg_list, f"{tmp_path}/export", "--statistics"])
    db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
    db.connect()
    expected = (
        db.cursor()
        .execute(
            "SELECT table_name FROM information_schema.tables "
            "WHERE table_name LIKE 'psm_test__psm_encounter_covariate_%'"
        )
        .fetchall()
    )
    assert len(expected) == 2


@mock.patch.dict(
    os.environ,
    clear=True,
)
@pytest.mark.parametrize(
    "args,status,login_error,raises",
    [
        (["upload"], 204, False, pytest.raises(SystemExit)),
        (["upload", "--user", "user", "--id", "id"], 204, False, pytest.raises(SystemExit)),
        (
            ["upload", "--user", "user", "--id", "id"],
            500,
            False,
            pytest.raises(SystemExit),
        ),
        (
            ["upload", "--user", "baduser", "--id", "badid"],
            204,
            True,
            pytest.raises(SystemExit),
        ),
        (
            [
                "upload",
                "--user",
                "user",
                "--id",
                "id",
                "--target",
                "upload",
                str(pathlib.Path(__file__).resolve().parent / "test_data"),
            ],
            204,
            False,
            does_not_raise(),
        ),
        (
            [
                "upload",
                "--user",
                "user",
                "--id",
                "id",
                "--target",
                "upload_no_date",
                str(pathlib.Path(__file__).resolve().parent / "test_data"),
            ],
            204,
            False,
            pytest.raises(SystemExit),
        ),
    ],
)
@responses.activate
def test_cli_upload_studies(args, status, login_error, raises):
    with raises:
        if login_error:
            responses.add(responses.POST, "https://upload.url.test/upload/", status=401)
        else:
            responses.add(
                responses.POST,
                "https://upload.url.test/upload/",
                json={"url": "https://presigned.url.test", "fields": {"a": "b"}},
            )
        responses.add(responses.POST, "https://presigned.url.test", status=status)
        cli.main(cli_args=[*args, "--url", "https://upload.url.test/upload/"])


@mock.patch("cumulus_library.actions.uploader.upload_files")
def test_cli_upload_error(mock_upload):
    with pytest.raises(SystemExit):
        mock_upload.side_effect = requests.RequestException()
        cli.main(cli_args=["upload"])


@pytest.mark.parametrize(
    "args,calls,raises",
    [
        (["upload", "--user", "user", "--id", "id"], 0, pytest.raises(SystemExit)),
        (
            ["upload", "--user", "user", "--id", "id", "-t", "upload"],
            1,
            does_not_raise(),
        ),
        (
            ["upload", "--user", "user", "--id", "id", "-t", "not_found"],
            0,
            pytest.raises(SystemExit),
        ),
    ],
)
@mock.patch.dict(
    os.environ,
    clear=True,
)
@mock.patch("cumulus_library.actions.uploader.upload_data")
def test_cli_upload_filter(mock_upload_data, args, calls, raises):
    with raises:
        cli.main(
            cli_args=[
                *args,
                "--url",
                "https://upload.url.test/upload/",
                str(pathlib.Path(__file__).resolve().parent / "test_data"),
            ]
        )
        if len(mock_upload_data.call_args_list) == 1:
            target = args[args.index("-t") + 1]
            # filepath is in the third argument position in the upload data arg list
            assert target in str(mock_upload_data.call_args[0][2])
        assert mock_upload_data.call_count == calls


@mock.patch.dict(
    os.environ,
    clear=True,
)
@pytest.mark.parametrize("mode", ["cli", "env"])
# early exit with a dumb error
@mock.patch("cumulus_library.base_utils.StudyConfig", side_effect=ZeroDivisionError)
def test_cli_umls_parsing(mock_config, mode, tmp_path):
    with pytest.raises(ZeroDivisionError):
        match mode:
            case "cli":
                cli.main(
                    cli_args=duckdb_args(
                        ["build", "--umls-key=MY-KEY", "-t", "study_valid"], tmp_path
                    )
                )
            case "env":
                with mock.patch.dict(os.environ, {"UMLS_API_KEY": "MY-KEY"}):
                    cli.main(cli_args=duckdb_args(["build", "-t", "study_valid"], tmp_path))

    assert mock_config.call_args[1]["umls_key"] == "MY-KEY"


def test_cli_single_builder(tmp_path):
    cli.main(cli_args=duckdb_args(["build", "--builder=patient", "--target=core"], tmp_path))
    db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
    db.connect()
    tables = [x[0] for x in db.cursor().execute("show tables").fetchall()]
    for table in [
        "core__patient",
        "core__patient_ext_ethnicity",
        "core__patient_ext_race",
    ]:
        assert table in tables


def test_cli_finds_study_from_manifest_prefix(tmp_path):
    # This study is located inside a folder called `study_different_dir`,
    # but we're going to find it using its real study prefix from the manifest.
    cli.main(
        cli_args=duckdb_args(
            ["build", "-s", "tests/test_data", "--target=study_different_name"],
            tmp_path,
        )
    )
    db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
    db.connect()
    tables = {x[0] for x in db.cursor().execute("show tables").fetchall()}
    assert "study_different_name__table" in tables


@pytest.mark.parametrize(
    "option,raises",
    [
        ("foo:bar", does_not_raise()),
        ("foo", pytest.raises(SystemExit)),
    ],
)
@mock.patch("cumulus_library.base_utils.StudyConfig")
def test_cli_custom_args(mock_config, tmp_path, option, raises):
    mock_config.return_value.stats_clean = False
    mock_config.return_value.db.db_type = "duckdb"
    mock_config.return_value.schema = "main"
    mock_config.return_value.build_type = "default"
    with raises:
        cli.main(
            cli_args=duckdb_args(
                [
                    "build",
                    "-t",
                    "study_valid",
                    "-s",
                    f"{Path(__file__).resolve().parents[0]}/test_data/study_valid",
                    "-o",
                    option,
                ],
                tmp_path,
            )
        )
        called_options = mock_config.call_args[1]["options"]
        assert called_options[option.split(":")[0]] == option.split(":")[1]


@mock.patch("cumulus_library.base_utils.StudyConfig")
def test_cli_no_custom_args_yields_empty_dict(mock_config, tmp_path):
    mock_config.return_value.stats_clean = False
    mock_config.return_value.db.db_type = "duckdb"
    mock_config.return_value.schema = "main"
    mock_config.return_value.build_type = "default"
    cli.main(
        cli_args=duckdb_args(
            [
                "build",
                "-t",
                "study_valid",
                "-s",
                f"{Path(__file__).resolve().parents[0]}/test_data/study_valid",
            ],
            tmp_path,
        )
    )
    called_options = mock_config.call_args[1]["options"]
    assert {} == called_options


def test_cli_import_study(tmp_path):
    test_data = {"string": ["a", "b", None]}
    df = pandas.DataFrame(test_data)
    (tmp_path / "archive").mkdir()
    df.to_parquet(tmp_path / "archive/test__table.parquet")
    df.to_csv(tmp_path / "archive/test__table.csv")
    with zipfile.ZipFile(tmp_path / "archive/test.zip", "w") as archive:
        archive.write(tmp_path / "archive/test__table.parquet")
        archive.write(tmp_path / "archive/test__table.csv")
    (tmp_path / "archive/test__table.parquet").unlink()
    (tmp_path / "archive/test__table.csv").unlink()

    cli.main(
        cli_args=duckdb_args(
            [
                "import",
                "-a",
                str(tmp_path / "archive/test.zip"),
            ],
            tmp_path,
        )
    )


def test_dedicated_schema(tmp_path):
    core_build_args = duckdb_args(
        [
            "build",
            "-t",
            "core",
        ],
        tmp_path,
    )
    build_args = duckdb_args(
        [
            "build",
            "-t",
            "study_dedicated_schema",
            "-s",
            "tests/test_data/study_dedicated_schema/",
        ],
        tmp_path,
    )
    cli.main(cli_args=core_build_args)
    cli.main(cli_args=build_args)
    db = databases.DuckDatabaseBackend(f"{tmp_path}/duck.db")
    db.connect()
    tables = (
        db.cursor()
        .execute("SELECT table_schema,table_name FROM information_schema.tables")
        .fetchall()
    )
    for table in [
        ("dedicated", "table_1"),
        ("dedicated", "view_2"),
        ("dedicated", "table_raw_sql"),
        ("main", "core__condition"),
    ]:
        assert table in tables


@mock.patch("cumulus_library.databases.duckdb.DuckDatabaseBackend")
def test_sql_error_handling(mock_backend, tmp_path):
    mock_backend.return_value.cursor.return_value.execute.side_effect = [
        None,
        Exception("bad query"),
    ]
    build_args = duckdb_args(
        [
            "build",
            "-t",
            "study_valid",
            "-s",
            "tests/test_data/study_valid/",
        ],
        tmp_path,
    )
    with pytest.raises(SystemExit):
        cli.main(cli_args=build_args)


def test_version(capfd):
    out = None
    with pytest.raises(SystemExit):
        cli.main(
            cli_args=[
                "version",
                "-s",
                "tests/test_data/study_valid/",
                "-s",
                # TODO: Stand in for a 'study without an __init__.py defining a version
                # consider a dedicated 'study_invalid_no_init' if we want to update all
                # other test studies to have an __init__.py
                "tests/test_data/study_invalid_bad_query/",
            ]
        )
    out, _ = capfd.readouterr()
    assert f"cumulus-library version: {__version__}" in out
    out = out.split("\n")
    valid = list(filter(lambda x: "study_valid " in x, out))
    assert len(valid) == 1
    assert "1.0.0 " in valid[0]
    invalid = list(filter(lambda x: "study_invalid_bad_query " in x, out))
    assert len(invalid) == 1
    assert "No version defined " in invalid[0]


@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_study_dir(tmp_path):
    os.environ["CUMULUS_LIBRARY_STUDY_DIR"] = str(
        pathlib.Path(__file__).resolve().parent / "test_data/"
    )
    build_args = duckdb_args(
        [
            "build",
            "-t",
            "study_valid",
        ],
        tmp_path,
    )
    with does_not_raise():
        cli.main(cli_args=build_args)


@pytest.mark.parametrize(
    "study,expected_queries,generated_query,toml_file, raises",
    [
        (
            "study_valid",
            [
                "stage_1.test.00.study_valid__table.sql",
                "stage_1.test2.00.study_valid__table2.sql",
            ],
            "CREATE TABLE study_valid__table (test int)",
            None,
            does_not_raise(),
        ),
        (
            "study_python_counts_valid",
            [
                "stage_1.module1.00.study_python_counts_valid__table1.sql",
                "stage_1.module1.00.study_python_counts_valid__table1.sql",
                "stage_1.module2.00.study_python_counts_valid__table2.sql",
            ],
            "CREATE TABLE IF NOT EXISTS study_python_counts_valid__table1 (test int);",
            None,
            does_not_raise(),
        ),
        (
            "psm_test",
            ["stage_1.psm_cohort.00.psm_test__psm_cohort.sql"],
            """CREATE TABLE psm_test__psm_cohort AS (
    SELECT * FROM core__condition --noqa: AM04
    ORDER BY id DESC LIMIT 100
)""",
            [
                "stage_1.psm_config.00.config.toml",
                """config_type = "psm"
classification_json = "classifications.json"
pos_source_table = "psm_test__psm_cohort"
neg_source_table = "core__condition"
target_table = "psm_test__psm_encounter_covariate"
primary_ref = 'encounter_ref'
count_ref = 'subject_ref'
count_table = 'core__condition'
dependent_variable = "example_diagnosis"
pos_sample_size = 20
neg_sample_size = 100
seed = 0
[join_cols_by_table.core__encounter]
join_id = "encounter_ref"
included_cols = [
    ["gender"], 
    ["race_display", "race"]
]
""",
            ],
            does_not_raise(),
        ),
        ("core", [], "", None, pytest.raises(SystemExit)),
    ],
)
def test_prepare_study(tmp_path, study, expected_queries, generated_query, toml_file, raises):
    with raises:
        build_args = duckdb_args(
            [
                "build",
                "-t",
                "core",
                str(tmp_path),
            ],
            tmp_path,
        )
        build_args = duckdb_args(
            [
                "build",
                "-t",
                study,
                "-s",
                "tests/test_data/",
                "--prepare",
                str(tmp_path),
            ],
            tmp_path,
        )
        with does_not_raise():
            cli.main(cli_args=build_args)
        queries = sorted(pathlib.Path(tmp_path).glob("**/*.sql"))
        for query in queries:
            assert query.name in expected_queries
        with open(queries[0]) as f:
            assert f.read() == generated_query
        if toml_file:
            config = next(pathlib.Path(tmp_path).glob("**/*.toml"))
            assert config.name == toml_file[0]
            with open(config) as f:
                assert f.read() == toml_file[1]


@mock.patch("concurrent.futures.ThreadPoolExecutor")
@mock.patch("botocore.session.Session")
@mock.patch("cumulus_library.databases.athena.AthenaDatabaseBackend")
@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_max_concurrent(mock_backend, mock_session, mock_threadpool, tmp_path):
    mock_backend.return_value.db_type = "athena"
    study_args = [
        "build",
        "-t",
        "study_valid_parallel",
        str(tmp_path),
        "-s",
        "tests/test_data/",
    ]
    build_args = duckdb_args(
        study_args,
        tmp_path,
    )
    cli.main(cli_args=build_args)
    assert mock_threadpool.call_args == mock.call(max_workers=20)
    cli.main(cli_args=[*build_args, "-c", "10"])
    assert mock_threadpool.call_args == mock.call(max_workers=10)
    cli.main(cli_args=study_args)
    assert mock_backend.call_args == mock.call("us-east-1", "cumulus", "default", None, None)
    cli.main(cli_args=[*study_args, "-c", "10"])
    assert mock_backend.call_args == mock.call("us-east-1", "cumulus", "default", None, 10)


@mock.patch("cumulus_library.cli._get_args")
@mock.patch.dict(
    os.environ,
    clear=True,
)
def test_env_vs_arg_handling(mock_args, tmp_path):
    os_path = tmp_path / "os"
    arg_path = tmp_path / "arg"
    os_path.mkdir(exist_ok=True, parents=True)
    arg_path.mkdir(exist_ok=True, parents=True)
    with pytest.raises(SystemExit):
        cli.main(cli_args=["version"])
        assert mock_args.call_args[0][0]["study_dir"] is None
    with pytest.raises(SystemExit):
        cli.main(cli_args=["version", "-s", str(arg_path)])
        assert mock_args.call_args[0][0]["study_dir"][0].name == "arg"
    with pytest.raises(SystemExit):
        os.environ.study_dir = os_path
        cli.main(cli_args=["version"])
        assert mock_args.call_args[0][0]["study_dir"][0].name == "os"
    with pytest.raises(SystemExit):
        assert os.environ.study_dir == os_path
        cli.main(cli_args=["version", "-s", str(arg_path)])
        assert mock_args.call_args[0][0]["study_dir"][0].name == "arg"
